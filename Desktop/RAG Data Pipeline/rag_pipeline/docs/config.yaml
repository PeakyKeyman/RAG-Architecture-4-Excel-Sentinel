# RAG Pipeline Configuration
# This file provides default settings that can be overridden via environment variables

# Embedding Model Configuration
embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  device: "cpu"
  batch_size: 32
  max_sequence_length: 512

# Vector Store Configuration  
vector_store:
  project_id: ""  # Set via GOOGLE_CLOUD_PROJECT
  location: "us-central1"
  index_endpoint_name: "rag-pipeline-index"
  deployed_index_id: "rag-pipeline-deployed"
  embedding_dimension: 1024
  distance_measure: "DOT_PRODUCT_DISTANCE"

# Chunking Configuration
chunking:
  child_chunk_size: 250
  parent_chunk_size: 750
  chunk_overlap: 0.15
  tokenizer_model: "cl100k_base"

# Cache Configuration
cache:
  max_size: 10000
  ttl_seconds: 3600

# Reranker Configuration
reranker:
  type: "cohere"
  model_name: "rerank-english-v3.0"
  max_chunks_per_query: 1000
  api_base_url: "https://api.cohere.ai/v1"

# HyDE Configuration
hyde:
  enabled: true
  num_hypothetical_docs: 3
  models:
    - "gemini-1.5-flash"
    - "gemini-1.5-pro"
  temperature: 0.7
  max_tokens: 150

# Search Configuration
search:
  hybrid_fusion_weight: 0.6  # Weight for vector search vs text search
  rrf_constant: 60  # Reciprocal Rank Fusion constant
  default_top_k: 20
  max_top_k: 100

# Context Packaging
context:
  max_context_tokens: 8000
  include_parent_chunks: true
  chunk_separator: "\n\n---\n\n"
  metadata_fields: ["source", "document_id", "chunk_type"]

# Knowledge Base Adjustment
knowledge_base:
  adjustment_enabled: true
  feedback_window_size: 100
  min_feedback_count: 5
  relevance_threshold: 0.7
  adjustment_factor: 0.1

# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  reload: false
  workers: 4
  log_level: "info"
  cors_origins: ["*"]
  rate_limit_requests_per_minute: 60

# Authentication
auth:
  enabled: false
  api_key: ""  # Set via API_KEY environment variable

# Logging Configuration
logging:
  level: "INFO"
  format: "json"
  include_timestamp: true
  include_caller: true

# Performance Configuration
performance:
  request_timeout_seconds: 30
  max_concurrent_requests: 10
  vector_search_timeout_seconds: 10
  rerank_timeout_seconds: 15

# Evaluation Configuration
evaluation:
  metrics: ["faithfulness", "answer_relevancy", "context_relevancy"]
  sample_size: 100
  langsmith_project: "rag-pipeline-eval"
  ragas_batch_size: 10

# Adaptive RAG Configuration
adaptive_rag:
  enabled: true
  simple_query_optimization: true
  complex_query_ensemble: true
  performance_monitoring: true
  fallback_to_default: true
  
  # Performance targets (milliseconds)
  simple_query_max_ms: 500
  medium_query_max_ms: 2000
  complex_query_max_ms: 8000
  
  # Query classifier settings
  classifier_logging: false
  
# Temporal Understanding Configuration
temporal:
  enabled: true
  fiscal_year_start: "January"  # Can be customized per organization
  relevance_decay_months: 12
  
  # Temporal scoring strategy
  scoring_strategy: "balanced"  # balanced, recency_boost, relevance_match, business_context
  
  # Recency boost parameters
  recency_boost_factor: 0.3
  current_boost: 1.5
  recent_boost: 1.2
  historical_penalty: 0.8
  archived_penalty: 0.5
  
  # Document type temporal weights
  strategy_recency_weight: 0.8
  financial_recency_weight: 0.9
  policy_recency_weight: 0.4
  operational_recency_weight: 0.6
  
  # Intent matching boosts
  temporal_intent_boost: 0.2
  comparative_boost: 0.15
  
  # Business context boosts
  quarterly_report_boost: 0.1
  annual_report_boost: 0.15
  board_meeting_boost: 0.1

# Development Configuration
development:
  debug: false
  hot_reload: false
  mock_external_apis: false
  log_request_bodies: false